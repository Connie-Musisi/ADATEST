#samdata = as.data.frame(sample_data(simdata_filter)[[group_var]])
# Subset based on the group variable directly for group 1
#simdata_group1 <- subset_samples(simdata_filter, samdata == group_levels[1])
#simdata_group1 <- subset_samples(simdata_filter, get(group_var) == group_levels[1])
samdata <- as.data.frame(sample_data(simdata_filter))
grp_vec <- samdata[[group_var]]
# Logical vector to subset sample names
samps_grp1 <- rownames(samdata)[grp_vec == group_levels[1]]
simdata_group1 <- prune_samples(samps_grp1, simdata_filter)
if (taxa_are_rows(simdata_group1)) {
otutab_group1 <- t(otu_table(simdata_group1))
} else {
otutab_group1 <- otu_table(simdata_group1)
}
n_samples_group1 <- nrow(otutab_group1)
# Subset for group 2 using the same direct approach
#simdata_group2 <- subset_samples(simdata_filter, samdata == group_levels[2])
#simdata_group2 <- subset_samples(simdata_filter, get(group_var) == group_levels[2])
samps_grp2 <- rownames(samdata)[grp_vec == group_levels[2]]
simdata_group2 <- prune_samples(samps_grp2, simdata_filter)
if (taxa_are_rows(simdata_group2)) {
otutab_group2 <- t(otu_table(simdata_group2))
} else {
otutab_group2 <- otu_table(simdata_group2)
}
n_samples_group2 <- nrow(otutab_group2)
n.taxa <- ncol(otutab_group1)
if (is.null(n.taxa0)) {
n.taxa0 <- round(n.taxa / 2)
}
# Get the taxon names from the OTU table
taxon_names <- taxa_names(simdata_group1)
taxon_name_pairs <- c()
use <- nrow(otutab_group1)
for (i in 1:(n.taxa - 1)) {
pairotu_subgroup1 <- t(matrix(c(otutab_group1[, i]), nrow = n.taxa - i, ncol = use, byrow = TRUE))
pairotu_subgroup2 <- otutab_group1[, (i + 1):n.taxa]
# Create the taxon name pairs
taxon_name_pairs <- c(taxon_name_pairs, paste(rep(taxon_names[i], n.taxa - i), taxon_names[(i + 1):n.taxa], sep = "-"))
if (i == 1) {
combined_paired_otu <- rbind(pairotu_subgroup1, pairotu_subgroup2)
} else {
combined_paired_otu <- cbind(combined_paired_otu, rbind(pairotu_subgroup1, pairotu_subgroup2))
}
}
# Remove taxa with too many zeroes
idx.keep.freq0 <- apply(combined_paired_otu, 2, function(x) { mean(x == 0) }) < 0.8
idx.keep.avg <- apply(combined_paired_otu, 2, function(x) { mean(x) }) > 0.001
idx.keep <- idx.keep.avg & idx.keep.freq0
combined_paired_otu <- combined_paired_otu[, idx.keep]
# Construct a New Phyloseq Object
tax_data <- tax_table(as.matrix(taxon_name_pairs[idx.keep]))
rownames(tax_data) <- taxon_name_pairs[idx.keep]
colnames(tax_data) <- c("Pseudo_taxon")
# Duplicate the sample data for the new structure
samdata_new <- sample_data(simdata_group1)
samdata_new <- as.data.frame(rbind(samdata_new, samdata_new))
samdata_new <- sample_data(samdata_new)
# Create a new OTU table with the combined paired OTU data (include names)
otu_table_new <- otu_table(combined_paired_otu, taxa_are_rows = FALSE)
colnames(otu_table_new) <- taxon_name_pairs[idx.keep]
row.names(otu_table_new) <- row.names(samdata_new)
training_dataset_ps <- phyloseq(otu_table_new, tax_data, samdata_new)
training_data <- phyloseq(otu_table_new, tax_data, samdata_new)
# Using empirical distribution to downsize group 2 training data to original
if (empirical_adjust) {
train_group2 <- otu_table_new[(n_samples_group1 + 1):(2 * n_samples_group1), ]
redtrain_group2 <- matrix(nrow = n_samples_group2, ncol = ncol(train_group2))
apply_emp <- function(x) {
set.seed(19)
emp_dist <- ecdf(x)
quantile(emp_dist, probs = runif(n_samples_group2))
}
redtrain_group2 <- apply(train_group2, 2, apply_emp)
train_otu_table <- otu_table(rbind(otu_table_new[1:n_samples_group1, ],
redtrain_group2), taxa_are_rows = FALSE)
rownames(train_otu_table) <- rownames(simdata_filter@sam_data)
train_sample_data <- sample_data(simdata_filter)[, "group"]
training_data <- phyloseq(train_otu_table, tax_data, train_sample_data)
}
# Log fold change calculation
otu_train_filter <- training_data@otu_table
data_group1 <- otu_train_filter[1:n_samples_group1, ]
data_group2 <- otu_train_filter[(n_samples_group1 + 1):(n_samples_group1 + n_samples_group2), ]
mean_group1 <- colMeans(data_group1)
mean_group2 <- colMeans(data_group2)
min.without.zero <- apply(otu_train_filter, 2, function(x) { min(x[x != 0]) })
log_fold_change <- log2((mean_group2 + min.without.zero) / (mean_group1 + min.without.zero))
# Calculate δ_med as the median of the log fold changes
delta_med <- median(log_fold_change, na.rm = TRUE)
# Center the log fold changes
lfc_centered <- log_fold_change - delta_med
# Extract the taxonomic table as a data frame
tax_df <- as.data.frame(tax_table(training_data))
tax_df$lfc <- log_fold_change
tax_df$lfc_centered <- lfc_centered
# Assign group indicators based on LFC using thresholds t0, t1, t2
tax_df$group_ind <- ifelse(abs(lfc_centered) < t0, "I_0",
ifelse(abs(lfc_centered) > t1 & abs(lfc_centered) < t2, "I_1", "I_B"))
sample_df <- as.data.frame(sample_data(training_data))
sample_df$new_group <- c(rep(0, n_samples_group1), rep(1, n_samples_group2))
sample_data_new <- sample_data(sample_df)
train_final <- phyloseq(otu_table(training_data), sample_data_new, tax_table(as.matrix(tax_df)))
taxa_I0 <- tax_df$Pseudo_taxon[tax_df$group_ind == "I_0"]
taxa_I1 <- tax_df$Pseudo_taxon[tax_df$group_ind == "I_1"]
train_taxa <- c(taxa_I0, taxa_I1)
train_pruned <- prune_taxa(train_taxa, train_final)
return(list(pseudo_lfc = log_fold_change,
delta_med = delta_med,
lfc_centered = lfc_centered,
Train = train_pruned))
}
# Subset data for training
Pseudo_Train <- PseudoData(simdata_filter, group_var = group_var,
group_levels = group_levels,
t0 = t0, t1, n.taxa0 = n.taxa0, t2,
empirical_adjust = empirical_adjust)
Train <- Pseudo_Train$Train
View(Train)
View(Train@tax_table)
Train_nan <- NA.remove(Train)
Train_res <- Train_analyze(Train_nan)
# Permutation Test
cat("running permutation test")
Orig_results <- Orig_Alz_Perm(simdata_filter, group_var = group_var,
group_levels = group_levels,
Train_parest = Train_res$Train_parest,
p.adjust.method = p.adjust.method, B = B)
Normalize<-function(sample1,sample2) {
pooled<-c(sample1,sample2)
q<-quantile(pooled,probs=c(0,0.99))
norm.factor<-max((q[2]-q[1]),max(pooled)/2) # in case of many zeroes
sample1_norm<-(sample1-q[1])/norm.factor
sample2_norm<-(sample2-q[1])/norm.factor
# version 2 needs following three lines
m<-(median(as.numeric(sample2))+median(as.numeric(sample1)))/2
sample1_norm<-sample1_norm-m
sample2_norm<-sample2_norm-m
return(list(
sample1=sample1_norm,
sample2=sample2_norm
))
}
PseudoData <- function(simdata_filter,
group_var,
group_levels,
t0=1,
t1=1.5,
t2=+Inf,
empirical_adjust = TRUE,
n.taxa0 = NULL) {
# Convert the sample data to a data frame for safe subsetting
#samdata = as.data.frame(sample_data(simdata_filter)[[group_var]])
# Subset based on the group variable directly for group 1
#simdata_group1 <- subset_samples(simdata_filter, samdata == group_levels[1])
#simdata_group1 <- subset_samples(simdata_filter, get(group_var) == group_levels[1])
samdata <- as.data.frame(sample_data(simdata_filter))
grp_vec <- samdata[[group_var]]
# Logical vector to subset sample names
samps_grp1 <- rownames(samdata)[grp_vec == group_levels[1]]
simdata_group1 <- prune_samples(samps_grp1, simdata_filter)
if (taxa_are_rows(simdata_group1)) {
otutab_group1 <- t(otu_table(simdata_group1))
} else {
otutab_group1 <- otu_table(simdata_group1)
}
n_samples_group1 <- nrow(otutab_group1)
# Subset for group 2 using the same direct approach
#simdata_group2 <- subset_samples(simdata_filter, samdata == group_levels[2])
#simdata_group2 <- subset_samples(simdata_filter, get(group_var) == group_levels[2])
samps_grp2 <- rownames(samdata)[grp_vec == group_levels[2]]
simdata_group2 <- prune_samples(samps_grp2, simdata_filter)
if (taxa_are_rows(simdata_group2)) {
otutab_group2 <- t(otu_table(simdata_group2))
} else {
otutab_group2 <- otu_table(simdata_group2)
}
n_samples_group2 <- nrow(otutab_group2)
n.taxa <- ncol(otutab_group1)
if (is.null(n.taxa0)) {
n.taxa0 <- round(n.taxa / 2)
}
# Get the taxon names from the OTU table
taxon_names <- taxa_names(simdata_group1)
taxon_name_pairs <- c()
use <- nrow(otutab_group1)
for (i in 1:(n.taxa - 1)) {
pairotu_subgroup1 <- t(matrix(c(otutab_group1[, i]), nrow = n.taxa - i, ncol = use, byrow = TRUE))
pairotu_subgroup2 <- otutab_group1[, (i + 1):n.taxa]
# Create the taxon name pairs
taxon_name_pairs <- c(taxon_name_pairs, paste(rep(taxon_names[i], n.taxa - i), taxon_names[(i + 1):n.taxa], sep = "-"))
if (i == 1) {
combined_paired_otu <- rbind(pairotu_subgroup1, pairotu_subgroup2)
} else {
combined_paired_otu <- cbind(combined_paired_otu, rbind(pairotu_subgroup1, pairotu_subgroup2))
}
}
# Remove taxa with too many zeroes
idx.keep.freq0 <- apply(combined_paired_otu, 2, function(x) { mean(x == 0) }) < 0.8
idx.keep.avg <- apply(combined_paired_otu, 2, function(x) { mean(x) }) > 0.001
idx.keep <- idx.keep.avg & idx.keep.freq0
combined_paired_otu <- combined_paired_otu[, idx.keep]
# Construct a New Phyloseq Object
tax_data <- tax_table(as.matrix(taxon_name_pairs[idx.keep]))
rownames(tax_data) <- taxon_name_pairs[idx.keep]
colnames(tax_data) <- c("Pseudo_taxon")
# Duplicate the sample data for the new structure
samdata_new <- sample_data(simdata_group1)
samdata_new <- as.data.frame(rbind(samdata_new, samdata_new))
samdata_new <- sample_data(samdata_new)
# Create a new OTU table with the combined paired OTU data (include names)
otu_table_new <- otu_table(combined_paired_otu, taxa_are_rows = FALSE)
colnames(otu_table_new) <- taxon_name_pairs[idx.keep]
row.names(otu_table_new) <- row.names(samdata_new)
training_dataset_ps <- phyloseq(otu_table_new, tax_data, samdata_new)
training_data <- phyloseq(otu_table_new, tax_data, samdata_new)
# Using empirical distribution to downsize group 2 training data to original
if (empirical_adjust) {
train_group2 <- otu_table_new[(n_samples_group1 + 1):(2 * n_samples_group1), ]
redtrain_group2 <- matrix(nrow = n_samples_group2, ncol = ncol(train_group2))
apply_emp <- function(x) {
set.seed(19)
emp_dist <- ecdf(x)
quantile(emp_dist, probs = runif(n_samples_group2))
}
redtrain_group2 <- apply(train_group2, 2, apply_emp)
train_otu_table <- otu_table(rbind(otu_table_new[1:n_samples_group1, ],
redtrain_group2), taxa_are_rows = FALSE)
rownames(train_otu_table) <- rownames(simdata_filter@sam_data)
train_sample_data <- sample_data(simdata_filter)[, "group"]
training_data <- phyloseq(train_otu_table, tax_data, train_sample_data)
}
# Log fold change calculation
otu_train_filter <- training_data@otu_table
data_group1 <- otu_train_filter[1:n_samples_group1, ]
data_group2 <- otu_train_filter[(n_samples_group1 + 1):(n_samples_group1 + n_samples_group2), ]
mean_group1 <- colMeans(data_group1)
mean_group2 <- colMeans(data_group2)
min.without.zero <- apply(otu_train_filter, 2, function(x) { min(x[x != 0]) })
log_fold_change <- log2((mean_group2 + min.without.zero) / (mean_group1 + min.without.zero))
# Calculate δ_med as the median of the log fold changes
delta_med <- median(log_fold_change, na.rm = TRUE)
# Center the log fold changes
lfc_centered <- log_fold_change - delta_med
# Extract the taxonomic table as a data frame
tax_df <- as.data.frame(tax_table(training_data))
tax_df$lfc <- log_fold_change
tax_df$lfc_centered <- lfc_centered
# Assign group indicators based on LFC using thresholds t0, t1, t2
tax_df$group_ind <- ifelse(abs(lfc_centered) < t0, "I_0",
ifelse(abs(lfc_centered) > t1 & abs(lfc_centered) < t2, "I_1", "I_B"))
sample_df <- as.data.frame(sample_data(training_data))
sample_df$new_group <- c(rep(0, n_samples_group1), rep(1, n_samples_group2))
sample_data_new <- sample_data(sample_df)
train_final <- phyloseq(otu_table(training_data), sample_data_new, tax_table(as.matrix(tax_df)))
taxa_I0 <- tax_df$Pseudo_taxon[tax_df$group_ind == "I_0"]
taxa_I1 <- tax_df$Pseudo_taxon[tax_df$group_ind == "I_1"]
train_taxa <- c(taxa_I0, taxa_I1)
train_pruned <- prune_taxa(train_taxa, train_final)
return(list(pseudo_lfc = log_fold_change,
delta_med = delta_med,
lfc_centered = lfc_centered,
Train = train_pruned))
}
Train_analyze <- function(data, seed=set.seed(19)){
########## Train data ######
## ---------------------------------------------------
##### Permutation #####
# For I0 group
train_I0 <- subset_taxa(data, group_ind == "I_0")
otu_train_I0 <- otu_table(train_I0, taxa_are_rows = FALSE)
tax_train_I0 <- tax_table(train_I0)
n0<-nrow(otu_train_I0)/2
# For I1 group
train_I1 <- subset_taxa(data, group_ind == "I_1")
# Convert OTU Table for I0 to DataFrame
otu_train_I0 <- as.data.frame(otu_train_I0)
# Permute the taxa labels in the OTU table for the I0 group for reproducibility
Train_perm_I0<-otu_train_I0
colnames(Train_perm_I0) <- taxa_names(train_I0) # not necessary
rownames(Train_perm_I0) <- rownames(otu_train_I0)
# Convert back to Phyloseq OTU Table format
Train_otu_tab_I0 <- otu_table(Train_perm_I0, taxa_are_rows = FALSE)
train_I0_perm<-phyloseq(Train_otu_tab_I0,sample_data(data),tax_train_I0)
train_phy <- merge_phyloseq(train_I0_perm,train_I1)
## ---------------------------------------------------------------------------------------------------------------------
############### Linear REGRESSION MODEL #################
# 1. Extract data
otu_abundance1 <- otu_table(train_phy) # OTU abundance matrix
taxon_data_df1 <- data.frame(tax_table(train_phy))
outcome_variable1 <- taxon_data_df1$group_ind
# 2. Prepare data for regression
# Combine sample data with OTU abundance data
regression_data1 <- as.matrix(t(otu_abundance1))
test_func = function(x){regression_normalized<-Normalize(x[1:n0],
x[(n0+1):(2*n0)])
reg11 <- regression_normalized$sample1
reg21 <- regression_normalized$sample2
avg_reg11 <- mean(reg11)
avg_reg21 <- mean(reg21)
if(avg_reg21 < avg_reg11){
return(c(reg21, reg11))
}
else {
return(c(reg11, reg21))
}}
regression_data1 <- as.data.frame(t(apply(regression_data1,1,test_func)))
regression_data1 = t(apply(regression_data1,1,function(x){c(sort(as.numeric(x[1:n0]),
decreasing = FALSE),sort(as.numeric(x[c((n0+1):(2*n0))]),
decreasing = FALSE))}))
y <- numeric(nrow(regression_data1))
y[outcome_variable1=="I_0"] = -1 # used to be zero
y[outcome_variable1=="I_1"] = 1
x <- as.matrix(regression_data1)
if (any(is.na(y))) {
stop("There are NA values in y")
}
if (length(y) != nrow(x)) {
stop("The length of y2 does not match the number of rows in x1")
}
# Fitting linear model to train data
Train_parest<-t(x)%*%y/length(y)
Train_pred<-x%*%Train_parest
## ---------------------------------------------------------------------------------------------------------------------
# Predicting the outcome
Train_parest <- as.numeric(Train_parest)
# matrix with both original group and predicted group
colnames(Train_pred)[1] <- 'Predictions'
Train_results <- cbind(Train_pred, y)
# splitting the data into the 2 groups I_) and I_1 to get the distributions
Train_results <- as.data.frame(Train_results)
return(list(y=y,
Train_scaled=regression_data1,
Train_results=Train_results,
Train_parest=Train_parest))
}
NA.remove <- function(physeq) {
otu <- otu_table(physeq)
tax <- tax_table(physeq)
# Check if taxa are rows or columns and remove NA accordingly
if(taxa_are_rows(physeq)) {
# Identify taxa (rows) that don't contain any NA
valid_taxa <- !rowSums(is.na(otu))
# Subset the OTU and taxa tables to keep only valid taxa
otu <- otu[valid_taxa, ]
tax <- tax[valid_taxa, ]
} else {
# Identify taxa (columns) without any NA
valid_taxa <- !colSums(is.na(otu))
# Subset the OTU and taxa tables to keep only valid taxa
otu <- otu[, valid_taxa]
tax <- tax[valid_taxa, ]
}
# Put the new tables back into the phyloseq object
otu_table(physeq) <- otu
tax_table(physeq) <- tax
return(physeq)
}
Test_Statistic <- function(org_otu, Train_parest, n1 = nrow(org_otu) / 2,
n2 = nrow(org_otu) / 2, B = 100) {
n.taxa <- ncol(org_otu)
otu1 <- org_otu[1:n1, ]
otu2 <- org_otu[(n1 + 1):(n1 + n2), ]
taxa.means1 <- colMeans(otu1)
taxa.means2 <- colMeans(otu2)
# Identify group with smaller mean
smallest <- ifelse(taxa.means1 - taxa.means2 < 0, 1, 2)
# Sort within groups
otu1.sorted <- t(apply(otu1, 2, sort, decreasing = FALSE))
otu2.sorted <- t(apply(otu2, 2, sort, decreasing = FALSE))
# Align groups based on mean abundance
select1 <- (smallest == 1)
select2 <- (smallest == 2)
otu.final <- matrix(nrow = n.taxa, ncol = (n1 + n2))
otu.final[select1, (n1 + 1):(n1 + n2)] <- otu2.sorted[select1, ]
otu.final[select1, 1:n1] <- otu1.sorted[select1, ]
otu.final[select2, (n1 + 1):(n1 + n2)] <- otu1.sorted[select2, 1:n2]
otu.final[select2, 1:n2] <- otu2.sorted[select2, ]
# Compute test statistic using estimated parameters
org_pred <- otu.final %*% Train_parest
return(stats = org_pred)
}
Orig_Alz_Perm <- function(simdata_filter, Train_parest, B=1000,
p.adjust.method="BH", n1=nrow(org_otu)/2,
group_var,
group_levels,
n2=nrow(org_otu)/2){
#set.seed(19)
############# Analyzing Original data ################
org_data <- simdata_filter
Org_tax <- as.data.frame(tax_table(org_data))
if(taxa_are_rows(org_data)){
org_otu <- t(as.matrix(otu_table(org_data)))
}else{
org_otu <- as.matrix(otu_table(org_data))
}
samdata <- sample_data(simdata_filter)[,group_var]
n1 <- sum(samdata == group_levels[1])
n2 <- sum(samdata == group_levels[2])
n.taxa<-nrow(simdata_filter@tax_table)
tmp<-sapply(1:n.taxa, FUN=function(i) {
d<-Normalize(org_otu[1:n1,i],
org_otu[(n1+1):(n1+n2),i])
return(c(d$sample1,d$sample2))
})
otu1<-tmp[1:n1,]
otu2<-tmp[(n1+1):(n1+n2),]
otu_scaled<-rbind(otu1,otu2)
stat.obs<-Test_Statistic(otu_scaled,Train_parest,B=B)
set.seed(19)
# Permutation null distributions
nd<-matrix(nrow=B,ncol=n.taxa)
for(i in 1:B) {
perm_otu<-otu_scaled[sample(1:(n1+n2),(n1+n2),replace = FALSE),]
nd[i,]<-Test_Statistic(perm_otu,Train_parest,B=B)
}
p<-sapply(1:n.taxa, function(i) {
mean(nd[,i]>=stat.obs[i])
})
# P-value adjustment
if(p.adjust.method == "qvalue"){
check = tryCatch(qvalue(p=p)$qvalues, error = function(e) {
return(NULL)
})
if(is.null(check)){p.adjusted <- qvalue(p=p,lambda=0)$qvalues}
else{p.adjusted <-check}
} else {
p.adjusted <- p.adjust(p, method=p.adjust.method)
}
colnames(stat.obs)<-"stat.obs"
Original_results <- cbind(stat.obs,Org_tax)
## ----Results----------------------------------------------------------------------------------------------------------
new_ind <- character(nrow(Original_results))
new_ind[p.adjusted>=0.05] <- "I_0"
new_ind[p.adjusted<0.05] <- "I_1"
Original_results$new_ind <- new_ind
Original_results$p<-p
Original_results$p.adjusted<-p.adjusted
return(list(org_scaled=otu_scaled,
Org_tax=Org_tax,
p=p,
p.adjusted=p.adjusted,
Stat_Obs=stat.obs,
Original_results=Original_results))
}
eval <- function(Res, isDA=Res$isDA, new_isDA=Res$new_ind){
FP <- sum(isDA == "FALSE" & new_isDA == "I_1")
TP <- sum(isDA == "TRUE" & new_isDA == "I_1")
FN <- sum(isDA == "TRUE" & new_isDA == "I_0")
TN <- sum(isDA == "FALSE" & new_isDA == "I_0")
## ----FDR------------------------------------------
FDR <- FP/(FP+TP)
## ----Sensitivity----------------------------------
TPR <- TP/(TP+FN)
## ----Specificity----------------------------------
TNR <- TN/(TN+FP)
## ----Type 1 error rate----------------------------
Type_1_error <- FP/(FP+TN)
## ----Power----------------------------------------
beta <- FN/(FN+TP)
return(list(FDR=FDR,
Sensitivity=TPR,
Specificity=TNR,
Type_1_error=Type_1_error,
Power=1-beta))
}
ADATEST <- function(physeq, group_var = "group",
group_levels = c("0", "1"),
t0=1, t1=1.5, t2=+Inf, B=1000,
empirical_adjust = TRUE,
p.adjust.method = "BH",
n.taxa0=NULL){
# Total Sum Scaling
simdata_filter <- transform_sample_counts(physeq, function(x) { x / sum(x)})
# Trimming
simdata_filter <- signtrans::Trim(simdata_filter,minReads = 0.001, minPrev = 0.20)
simdata_filter <- transform_sample_counts(simdata_filter, function(x) { x / sum(x)})
cat("running DATA")
# Subset data for training
Pseudo_Train <- PseudoData(simdata_filter, group_var = group_var,
group_levels = group_levels,
t0 = t0, t1, n.taxa0 = n.taxa0, t2,
empirical_adjust = empirical_adjust)
Train <- Pseudo_Train$Train
Train_nan <- NA.remove(Train)
Train_res <- Train_analyze(Train_nan)
# Permutation Test
cat("running permutation test")
Orig_results <- Orig_Alz_Perm(simdata_filter, group_var = group_var,
group_levels = group_levels,
Train_parest = Train_res$Train_parest,
p.adjust.method = p.adjust.method, B = B)
Results <- eval(Orig_results$Original_results)
return(list(Original_results = Orig_results,
Unscaled_data = list(Train_data = Train_nan, Original_data = simdata_filter),
Train_results = Train_res,
Pseudo = list(Pseudo_Train$delta_med, Pseudo_Train$lfc_centered, Pseudo_Train$pseudo_lfc),
Results = Results))
}
load("G:/My Drive/PhD UHasselt Connie.zip (Unzipped Files)/Adaptive test/R code/Data/MIDASim/Sim_datasets/Dietswap_list/Setting4_n200_b1.5_rnt20.RData")
physeq = Setting4_n200_b1.5_rnt20[[1]]
res = ADATEST(physeq, group_var = "group",
group_levels = c("0", "1"),
t0=1, t1=1.5, t2=+Inf, B=1000,
empirical_adjust = TRUE,
p.adjust.method = "BH",
n.taxa0=NULL)
View(res)
library(devtools)
devtools::install_github("Connie-Musisi/ADATEST")
res = ADATEST(dietswap2, group_var="nationality",
group_levels= c("AFR", "AAM"))
