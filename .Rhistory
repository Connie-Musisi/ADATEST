# Using empirical distribution to downsize group 2 training data to original
if (empirical_adjust) {
train_group2 <- otu_table_new[(n_samples_group1 + 1):(2 * n_samples_group1), ]
redtrain_group2 <- matrix(nrow = n_samples_group2, ncol = ncol(train_group2))
apply_emp <- function(x) {
set.seed(19)
emp_dist <- ecdf(x)
quantile(emp_dist, probs = runif(n_samples_group2))
}
redtrain_group2 <- apply(train_group2, 2, apply_emp)
train_otu_table <- otu_table(rbind(otu_table_new[1:n_samples_group1, ],
redtrain_group2), taxa_are_rows = FALSE)
rownames(train_otu_table) <- rownames(simdata_filter@sam_data)
train_sample_data <- sample_data(simdata_filter)[, "group"]
training_data <- phyloseq(train_otu_table, tax_data, train_sample_data)
}
# Log fold change calculation
otu_train_filter <- training_data@otu_table
data_group1 <- otu_train_filter[1:n_samples_group1, ]
data_group2 <- otu_train_filter[(n_samples_group1 + 1):(n_samples_group1 + n_samples_group2), ]
mean_group1 <- colMeans(data_group1)
mean_group2 <- colMeans(data_group2)
min.without.zero <- apply(otu_train_filter, 2, function(x) { min(x[x != 0]) })
log_fold_change <- log2((mean_group2 + min.without.zero) / (mean_group1 + min.without.zero))
# If thresholds are set to "default", recalculate them based on log_fold_change
if (is.character(t0) && t0 == "default" &&
is.character(t1) && t1 == "default" &&
is.character(t2) && t2 == "default") {
t0 <- as.numeric(quantile(abs(log_fold_change), 0.1))
t1 <- as.numeric(quantile(abs(log_fold_change), 0.9))
t2 <- 10
if (t1 < 1.5 || t1 > t2) {
t1 <- max(1.5, t0 + 0.5)
}
}
# If thresholds are set to "default", recalculate them based on log_fold_change
if (is.character(t0) && t0 == "default" &&
is.character(t1) && t1 == "default" &&
is.character(t2) && t2 == "default") {
t0 <- as.numeric(quantile(abs(log_fold_change), 0.1))
t1 <- as.numeric(quantile(abs(log_fold_change), 0.9))
t2 <- 10
if (t1 < 1.5 || t1 > t2) {
t1 <- max(1.5, t0 + 0.5)
}
}else{
t0=t0
t1=t1
t2=t2
}
Data <- function(simdata_filter,
t0 = 1,
t1 = 1.5,
t2 = Inf,
n.taxa0 = NULL,
empirical_adjust = TRUE) {
# Extract samples from group 0 (the original group you want to match with downsizing)
simdata_group1 <- subset_samples(simdata_filter, group == 0)
if (taxa_are_rows(simdata_group1)) {
otutab_group1 <- t(otu_table(simdata_group1))
} else {
otutab_group1 <- otu_table(simdata_group1)
}
n_samples_group1 <- nrow(otutab_group1)
# Extract samples from group 1 (used to create the pseudo groups)
simdata_group2 <- subset_samples(simdata_filter, group == 1)
if (taxa_are_rows(simdata_group2)) {
otutab_group2 <- t(otu_table(simdata_group2))
} else {
otutab_group2 <- otu_table(simdata_group2)
}
n_samples_group2 <- nrow(otutab_group2)
n.taxa <- ncol(otutab_group1)
if (is.null(n.taxa0)) {
n.taxa0 <- round(n.taxa / 2)
}
# Get the taxon names from the OTU table
taxon_names <- taxa_names(simdata_group1)
taxon_name_pairs <- c()
use <- nrow(otutab_group1)
for (i in 1:(n.taxa - 1)) {
pairotu_subgroup1 <- t(matrix(c(otutab_group1[, i]), nrow = n.taxa - i, ncol = use, byrow = TRUE))
pairotu_subgroup2 <- otutab_group1[, (i + 1):n.taxa]
# Create the taxon name pairs
taxon_name_pairs <- c(taxon_name_pairs, paste(rep(taxon_names[i], n.taxa - i), taxon_names[(i + 1):n.taxa], sep = "-"))
if (i == 1) {
combined_paired_otu <- rbind(pairotu_subgroup1, pairotu_subgroup2)
} else {
combined_paired_otu <- cbind(combined_paired_otu, rbind(pairotu_subgroup1, pairotu_subgroup2))
}
}
# Remove taxa with too many zeroes
idx.keep.freq0 <- apply(combined_paired_otu, 2, function(x) { mean(x == 0) }) < 0.8
idx.keep.avg <- apply(combined_paired_otu, 2, function(x) { mean(x) }) > 0.001
idx.keep <- idx.keep.avg & idx.keep.freq0
combined_paired_otu <- combined_paired_otu[, idx.keep]
# Construct a New Phyloseq Object
tax_data <- tax_table(as.matrix(taxon_name_pairs[idx.keep]))
rownames(tax_data) <- taxon_name_pairs[idx.keep]
colnames(tax_data) <- c("Pseudo_taxon")
# Duplicate the sample data for the new structure
samdata_new <- sample_data(simdata_group1)
samdata_new <- as.data.frame(rbind(samdata_new, samdata_new))
samdata_new <- sample_data(samdata_new)
# Create a new OTU table with the combined paired OTU data (include names)
otu_table_new <- otu_table(combined_paired_otu, taxa_are_rows = FALSE)
colnames(otu_table_new) <- taxon_name_pairs[idx.keep]
row.names(otu_table_new) <- row.names(samdata_new)
training_dataset_ps <- phyloseq(otu_table_new, tax_data, samdata_new)
training_data <- phyloseq(otu_table_new, tax_data, samdata_new)
# Using empirical distribution to downsize group 2 training data to original
if (empirical_adjust) {
train_group2 <- otu_table_new[(n_samples_group1 + 1):(2 * n_samples_group1), ]
redtrain_group2 <- matrix(nrow = n_samples_group2, ncol = ncol(train_group2))
apply_emp <- function(x) {
set.seed(19)
emp_dist <- ecdf(x)
quantile(emp_dist, probs = runif(n_samples_group2))
}
redtrain_group2 <- apply(train_group2, 2, apply_emp)
train_otu_table <- otu_table(rbind(otu_table_new[1:n_samples_group1, ],
redtrain_group2), taxa_are_rows = FALSE)
rownames(train_otu_table) <- rownames(simdata_filter@sam_data)
train_sample_data <- sample_data(simdata_filter)[, "group"]
training_data <- phyloseq(train_otu_table, tax_data, train_sample_data)
}
# Log fold change calculation
otu_train_filter <- training_data@otu_table
data_group1 <- otu_train_filter[1:n_samples_group1, ]
data_group2 <- otu_train_filter[(n_samples_group1 + 1):(n_samples_group1 + n_samples_group2), ]
mean_group1 <- colMeans(data_group1)
mean_group2 <- colMeans(data_group2)
min.without.zero <- apply(otu_train_filter, 2, function(x) { min(x[x != 0]) })
log_fold_change <- log2((mean_group2 + min.without.zero) / (mean_group1 + min.without.zero))
# If thresholds are set to "default", recalculate them based on log_fold_change
# if (is.character(t0) && t0 == "default" &&
#   is.character(t1) && t1 == "default" &&
#    is.character(t2) && t2 == "default") {
t0 <- as.numeric(quantile(abs(log_fold_change), 0.1))
t1 <- as.numeric(quantile(abs(log_fold_change), 0.9))
t2 <- 10
if (t1 < 1.5 || t1 > t2) {
t1 <- max(1.5, t0 + 0.5)
}
#}else{
#  t0=t0
#  t1=t1
#  t2=t2
#}
# Extract the taxonomic table as a data frame
tax_df <- as.data.frame(tax_table(training_data))
tax_df$lfc <- log_fold_change
# Assign group indicators based on LFC using thresholds t0, t1, t2
tax_df$group_ind <- ifelse(abs(log_fold_change) < t0, "I_0",
ifelse((abs(log_fold_change) > t1), "I_1", "I_none"))
sample_df <- as.data.frame(sample_data(training_data))
sample_df$new_group <- c(rep(0, n_samples_group1), rep(1, n_samples_group2))
sample_data_new <- sample_data(sample_df)
train_final <- phyloseq(otu_table(training_data), sample_data_new, tax_table(as.matrix(tax_df)))
taxa_I0 <- tax_df$Pseudo_taxon[tax_df$group_ind == "I_0"]
taxa_I1 <- tax_df$Pseudo_taxon[tax_df$group_ind == "I_1"]
train_taxa <- c(taxa_I0, taxa_I1)
train_pruned <- prune_taxa(train_taxa, train_final)
return(list(pseudo_lfc = log_fold_change,
Train = train_pruned))
}
data2=Data(simdata_filter)
View(data2)
Data <- function(simdata_filter,
t0 = 1,
t1 = 1.5,
t2 = Inf,
n.taxa0 = NULL,
empirical_adjust = TRUE) {
# Extract samples from group 0 (the original group you want to match with downsizing)
simdata_group1 <- subset_samples(simdata_filter, group == 0)
if (taxa_are_rows(simdata_group1)) {
otutab_group1 <- t(otu_table(simdata_group1))
} else {
otutab_group1 <- otu_table(simdata_group1)
}
n_samples_group1 <- nrow(otutab_group1)
# Extract samples from group 1 (used to create the pseudo groups)
simdata_group2 <- subset_samples(simdata_filter, group == 1)
if (taxa_are_rows(simdata_group2)) {
otutab_group2 <- t(otu_table(simdata_group2))
} else {
otutab_group2 <- otu_table(simdata_group2)
}
n_samples_group2 <- nrow(otutab_group2)
n.taxa <- ncol(otutab_group1)
if (is.null(n.taxa0)) {
n.taxa0 <- round(n.taxa / 2)
}
# Get the taxon names from the OTU table
taxon_names <- taxa_names(simdata_group1)
taxon_name_pairs <- c()
use <- nrow(otutab_group1)
for (i in 1:(n.taxa - 1)) {
pairotu_subgroup1 <- t(matrix(c(otutab_group1[, i]), nrow = n.taxa - i, ncol = use, byrow = TRUE))
pairotu_subgroup2 <- otutab_group1[, (i + 1):n.taxa]
# Create the taxon name pairs
taxon_name_pairs <- c(taxon_name_pairs, paste(rep(taxon_names[i], n.taxa - i), taxon_names[(i + 1):n.taxa], sep = "-"))
if (i == 1) {
combined_paired_otu <- rbind(pairotu_subgroup1, pairotu_subgroup2)
} else {
combined_paired_otu <- cbind(combined_paired_otu, rbind(pairotu_subgroup1, pairotu_subgroup2))
}
}
# Remove taxa with too many zeroes
idx.keep.freq0 <- apply(combined_paired_otu, 2, function(x) { mean(x == 0) }) < 0.8
idx.keep.avg <- apply(combined_paired_otu, 2, function(x) { mean(x) }) > 0.001
idx.keep <- idx.keep.avg & idx.keep.freq0
combined_paired_otu <- combined_paired_otu[, idx.keep]
# Construct a New Phyloseq Object
tax_data <- tax_table(as.matrix(taxon_name_pairs[idx.keep]))
rownames(tax_data) <- taxon_name_pairs[idx.keep]
colnames(tax_data) <- c("Pseudo_taxon")
# Duplicate the sample data for the new structure
samdata_new <- sample_data(simdata_group1)
samdata_new <- as.data.frame(rbind(samdata_new, samdata_new))
samdata_new <- sample_data(samdata_new)
# Create a new OTU table with the combined paired OTU data (include names)
otu_table_new <- otu_table(combined_paired_otu, taxa_are_rows = FALSE)
colnames(otu_table_new) <- taxon_name_pairs[idx.keep]
row.names(otu_table_new) <- row.names(samdata_new)
training_dataset_ps <- phyloseq(otu_table_new, tax_data, samdata_new)
training_data <- phyloseq(otu_table_new, tax_data, samdata_new)
# Using empirical distribution to downsize group 2 training data to original
if (empirical_adjust) {
train_group2 <- otu_table_new[(n_samples_group1 + 1):(2 * n_samples_group1), ]
redtrain_group2 <- matrix(nrow = n_samples_group2, ncol = ncol(train_group2))
apply_emp <- function(x) {
set.seed(19)
emp_dist <- ecdf(x)
quantile(emp_dist, probs = runif(n_samples_group2))
}
redtrain_group2 <- apply(train_group2, 2, apply_emp)
train_otu_table <- otu_table(rbind(otu_table_new[1:n_samples_group1, ],
redtrain_group2), taxa_are_rows = FALSE)
rownames(train_otu_table) <- rownames(simdata_filter@sam_data)
train_sample_data <- sample_data(simdata_filter)[, "group"]
training_data <- phyloseq(train_otu_table, tax_data, train_sample_data)
}
# Log fold change calculation
otu_train_filter <- training_data@otu_table
data_group1 <- otu_train_filter[1:n_samples_group1, ]
data_group2 <- otu_train_filter[(n_samples_group1 + 1):(n_samples_group1 + n_samples_group2), ]
mean_group1 <- colMeans(data_group1)
mean_group2 <- colMeans(data_group2)
min.without.zero <- apply(otu_train_filter, 2, function(x) { min(x[x != 0]) })
log_fold_change <- log2((mean_group2 + min.without.zero) / (mean_group1 + min.without.zero))
if (t0 == 1 && t1 == 1.5 && t2 == Inf) {
t0 <- as.numeric(quantile(abs(log_fold_change), 0.1))
t1 <- as.numeric(quantile(abs(log_fold_change), 0.9))
t2 <- 10
if (t1 < 1.5 || t1 > t2) {
t1 <- max(1.5, t0 + 0.5)
}
}
# Extract the taxonomic table as a data frame
tax_df <- as.data.frame(tax_table(training_data))
tax_df$lfc <- log_fold_change
# Assign group indicators based on LFC using thresholds t0, t1, t2
tax_df$group_ind <- ifelse(abs(log_fold_change) < t0, "I_0",
ifelse((abs(log_fold_change) > t1), "I_1", "I_none"))
sample_df <- as.data.frame(sample_data(training_data))
sample_df$new_group <- c(rep(0, n_samples_group1), rep(1, n_samples_group2))
sample_data_new <- sample_data(sample_df)
train_final <- phyloseq(otu_table(training_data), sample_data_new, tax_table(as.matrix(tax_df)))
taxa_I0 <- tax_df$Pseudo_taxon[tax_df$group_ind == "I_0"]
taxa_I1 <- tax_df$Pseudo_taxon[tax_df$group_ind == "I_1"]
train_taxa <- c(taxa_I0, taxa_I1)
train_pruned <- prune_taxa(train_taxa, train_final)
return(list(pseudo_lfc = log_fold_change,
Train = train_pruned))
}
data2=Data(simdata_filter)
View(data2)
library(devtools)
create("ADATEST")
Package: ADATEST
Title: An Adaptive Test for Differential Abundance in Microbiome Studies
devtools::document()
devtools::document(ADATEST)
create("ADATEST")
create("ADATEST")
devtools::document()
devtools::check()
devtools::document("ADATEST")
devtools::check("ADATEST")
devtools::document("ADATEST")
devtools::check("ADATEST")
vignette("rd-other")
devtools::document("ADATEST")
devtools::check("ADATEST")
devtools::document("ADATEST")
devtools::check("ADATEST")
list.files()
License: MIT + file LICENSE
usethis::use_mit_license("Connie Musisi")
getwd()
remove.packages("ADATEST")
setwd("G:\\ Drive\\PhD UHasselt Connie.zip (Unzipped Files)\\Adaptive test\\R code\\R package")
setwd("G:/Drive/PhD UHasselt Connie.zip (Unzipped Files)/Adaptive test/R code/R package")
setwd("G:/Drive/PhD UHasselt Connie.zip (Unzipped Files)/Adaptive test/R code/R package")
file.exists("G:/Drive/PhD UHasselt Connie.zip (Unzipped Files)/Adaptive test/R code/R package")
setwd("G:\\My Drive\\PhD UHasselt Connie.zip (Unzipped Files)\\Adaptive test\\R code\\R package")
library(roxygen2)
library(devtools)
create("ADATEST")
devtools::document("ADATEST")
warnings()
devtools::check("ADATEST")
devtools::document("ADATEST")
devtools::check("ADATEST")
usethis::use_mit_license("Connie Musisi")
usethis::proj_get()
usethis::create_package("G:/My Drive/PhD UHasselt Connie.zip (Unzipped Files)/Adaptive test/R code/R package")
library(devtools)
devtools::build(ADATEST)
detach(ADATEST, unload = TRUE)
detach("ADATEST", unload = TRUE)
?remove.packages()
remove.packages("ADATEST")
devtools::build_vignettes("ADATEST")
devtools::build_vignettes(ADATEST)
usethis::use_vignette("Example")
devtools::build_vignettes()
devtools::document()  # Generate Rd files and update package metadata
browseVignettes("ADATEST")
library(devtools)
devtools::install_github("Connie-Musisi/ADATEST")
load("G:/My Drive/PhD UHasselt Connie.zip (Unzipped Files)/Adaptive test/R code/Data/MIDASim/Sim_datasets/Results_dietswap_new/adaptive_res_set8.RData")
load("G:/My Drive/PhD UHasselt Connie.zip (Unzipped Files)/Adaptive test/R code/Data/MIDASim/Sim_datasets/Results_dietswap_new/adaptive_res_set15.RData")
parest_list <- list()
# Loop over the elements in your dataset (assuming the 'i' indices are defined)
for (i in 1:length(adaptive_res_set15[["Setting15_n50_b5_rnt50"]])) {
parest <- adaptive_res_set15[["Setting15_n50_b5_rnt50"]][[i]][["Train_results"]][["Train_parest"]]
parest_list[[i]] <- parest
}
parest <- do.call(rbind, parest_list)
avg_parest <- colMeans(parest)
plot(avg_parest, xlab="Ranks", ylab="Scores", cex.label= 1.5, cex.axis= 1.5)
avg_parest1  <- avg_parest[1:25]
avg_parest2 <- avg_parest[26:50]
par(mfrow=c(2,1))
plot(avg_parest1, xlab="Ranks", ylab="Scores", cex.label= 1.7, cex.axis= 1.2)
plot(avg_parest2, xlab="Ranks", ylab="Scores", cex.label= 1.7, cex.axis= 1.2)
par(mfrow = c(2,1),    # 2 rows, 1 column layout
oma = c(0, 4, 0, 0),  # outer margins: bottom, left, top, right
mar = c(4, 2, 2, 1))  # inner margins for each plot
plot(avg_parest1, xlab = "Ranks", ylab = "", cex.lab = 1.7, cex.axis = 1.2)
plot(avg_parest2, xlab = "Ranks", ylab = "", cex.lab = 1.7, cex.axis = 1.2)
# Add shared y-axis label in outer left margin
mtext("Scores", side = 2, outer = TRUE, line = 2.5, cex = 2)
load("G:/My Drive/PhD UHasselt Connie.zip (Unzipped Files)/Adaptive test/R code/Data/MIDASim/Sim_datasets/Results_countibd_new/adaptive_res_set15.RData")
parest_list <- list()
# Loop over the elements in your dataset (assuming the 'i' indices are defined)
for (i in 1:length(adaptive_res_set15[["Setting15_n50_b5_rnt50"]])) {
parest <- adaptive_res_set15[["Setting15_n50_b5_rnt50"]][[i]][["Train_results"]][["Train_parest"]]
parest_list[[i]] <- parest
}
parest <- do.call(rbind, parest_list)
avg_parest <- colMeans(parest)
avg_parest1  <- avg_parest[1:25]
avg_parest2 <- avg_parest[26:50]
par(mfrow = c(2,1),    # 2 rows, 1 column layout
oma = c(0, 4, 0, 0),  # outer margins: bottom, left, top, right
mar = c(4, 2, 2, 1))  # inner margins for each plot
plot(avg_parest1, xlab = "Ranks", ylab = "", cex.lab = 1.7, cex.axis = 1.2)
plot(avg_parest2, xlab = "Ranks", ylab = "", cex.lab = 1.7, cex.axis = 1.2)
# Add shared y-axis label in outer left margin
mtext("Scores", side = 2, outer = TRUE, line = 2.5, cex = 2)
data("count.ibd")
# Determine shared y-axis limits across both datasets
ymin <- min(avg_parest1, avg_parest2)
ymax <- max(avg_parest1, avg_parest2)
par(mfrow = c(2,1),    # 2 rows, 1 column layout
oma = c(0, 4, 0, 0),  # outer margins: bottom, left, top, right
mar = c(4, 2, 2, 1))  # inner margins
# First plot with fixed y-limits
plot(avg_parest1, xlab = "Ranks", ylab = "", cex.lab = 1.7, cex.axis = 1.2,
ylim = c(ymin, ymax))
# Second plot with the same y-limits
plot(avg_parest2, xlab = "Ranks", ylab = "", cex.lab = 1.7, cex.axis = 1.2,
ylim = c(ymin, ymax))
# Add shared y-axis label
mtext("Scores", side = 2, outer = TRUE, line = 2.5, cex = 2)
load("G:/My Drive/PhD UHasselt Connie.zip (Unzipped Files)/Adaptive test/R code/Data/MIDASim/Sim_datasets/Results_dietswap_new/adaptive_res_set15.RData")
# Initialize an empty list to store the parameter estimates
parest_list <- list()
# Loop over the elements in your dataset (assuming the 'i' indices are defined)
for (i in 1:length(adaptive_res_set15[["Setting15_n50_b5_rnt50"]])) {
parest <- adaptive_res_set15[["Setting15_n50_b5_rnt50"]][[i]][["Train_results"]][["Train_parest"]]
parest_list[[i]] <- parest
}
parest <- do.call(rbind, parest_list)
avg_parest <- colMeans(parest)
avg_parest1  <- avg_parest[1:25]
avg_parest2 <- avg_parest[26:50]
# Determine shared y-axis limits across both datasets
ymin <- min(avg_parest1, avg_parest2)
ymax <- max(avg_parest1, avg_parest2)
par(mfrow = c(2,1),    # 2 rows, 1 column layout
oma = c(0, 4, 0, 0),  # outer margins: bottom, left, top, right
mar = c(4, 2, 2, 1))  # inner margins
# First plot with fixed y-limits
plot(avg_parest1, xlab = "Ranks", ylab = "", cex.lab = 1.7, cex.axis = 1.2,
ylim = c(ymin, ymax))
# Second plot with the same y-limits
plot(avg_parest2, xlab = "Ranks", ylab = "", cex.lab = 1.7, cex.axis = 1.2,
ylim = c(ymin, ymax))
# Add shared y-axis label
mtext("Scores", side = 2, outer = TRUE, line = 2.5, cex = 2)
# Initialize an empty list to store the parameter estimates
parest_list <- list()
# Loop over the elements in your dataset (assuming the 'i' indices are defined)
for (i in 1:length(adaptive_res_set15[["Setting15_n50_b5_rnt50"]])) {
parest <- adaptive_res_set15[["Setting15_n50_b5_rnt50"]][[i]][["Train_results"]][["Train_parest"]]
parest_list[[i]] <- parest
}
parest <- do.call(rbind, parest_list)
avg_parest <- colMeans(parest)
avg_parest1  <- avg_parest[1:25]
avg_parest2 <- avg_parest[26:50]
# Determine shared y-axis limits across both datasets
ymin <- min(avg_parest1, avg_parest2)
ymax <- max(avg_parest1, avg_parest2)
par(mfrow = c(2,1),    # 2 rows, 1 column layout
oma = c(0, 4, 0, 0),  # outer margins: bottom, left, top, right
mar = c(4, 2, 2, 1))  # inner margins
# First plot with fixed y-limits
plot(avg_parest1, xlab = "Ranks", ylab = "", cex.lab = 1.7, cex.axis = 1.2,
ylim = c(ymin, ymax))
# Second plot with the same y-limits
plot(avg_parest2, xlab = "Ranks", ylab = "", cex.lab = 1.7, cex.axis = 1.2,
ylim = c(ymin, ymax))
# Add shared y-axis label
mtext("Scores", side = 2, outer = TRUE, line = 2.5, cex = 2)
load("G:/My Drive/PhD UHasselt Connie.zip (Unzipped Files)/Adaptive test/R code/Data/MIDASim/Sim_datasets/Results_dietswap_new/adaptive_res_set15.RData")
# Initialize an empty list to store the parameter estimates
parest_list <- list()
# Loop over the elements in your dataset (assuming the 'i' indices are defined)
for (i in 1:length(adaptive_res_set15[["Setting15_n50_b5_rnt50"]])) {
parest <- adaptive_res_set15[["Setting15_n50_b5_rnt50"]][[i]][["Train_results"]][["Train_parest"]]
parest_list[[i]] <- parest
}
parest <- do.call(rbind, parest_list)
avg_parest <- colMeans(parest)
avg_parest1  <- avg_parest[1:25]
avg_parest2 <- avg_parest[26:50]
# Determine shared y-axis limits across both datasets
ymin <- min(avg_parest1, avg_parest2)
ymax <- max(avg_parest1, avg_parest2)
par(mfrow = c(2,1),    # 2 rows, 1 column layout
oma = c(0, 4, 0, 0),  # outer margins: bottom, left, top, right
mar = c(4, 2, 2, 1))  # inner margins
# First plot with fixed y-limits
plot(avg_parest1, xlab = "Ranks", ylab = "", cex.lab = 1.7, cex.axis = 1.2,
ylim = c(ymin, ymax))
# Second plot with the same y-limits
plot(avg_parest2, xlab = "Ranks", ylab = "", cex.lab = 1.7, cex.axis = 1.2,
ylim = c(ymin, ymax))
# Add shared y-axis label
mtext("Scores", side = 2, outer = TRUE, line = 2.5, cex = 2)
load("G:/My Drive/PhD UHasselt Connie.zip (Unzipped Files)/Adaptive test/R code/Data/MIDASim/Sim_datasets/Results_countibd_new/adaptive_res_set15.RData")
# Initialize an empty list to store the parameter estimates
parest_list <- list()
# Loop over the elements in your dataset (assuming the 'i' indices are defined)
for (i in 1:length(adaptive_res_set15[["Setting15_n50_b5_rnt50"]])) {
parest <- adaptive_res_set15[["Setting15_n50_b5_rnt50"]][[i]][["Train_results"]][["Train_parest"]]
parest_list[[i]] <- parest
}
parest <- do.call(rbind, parest_list)
avg_parest <- colMeans(parest)
avg_parest1  <- avg_parest[1:25]
avg_parest2 <- avg_parest[26:50]
# Determine shared y-axis limits across both datasets
ymin <- min(avg_parest1, avg_parest2)
ymax <- max(avg_parest1, avg_parest2)
par(mfrow = c(2,1),    # 2 rows, 1 column layout
oma = c(0, 4, 0, 0),  # outer margins: bottom, left, top, right
mar = c(4, 2, 2, 1))  # inner margins
# First plot with fixed y-limits
plot(avg_parest1, xlab = "Ranks", ylab = "", cex.lab = 1.7, cex.axis = 1.2,
ylim = c(ymin, ymax))
# Second plot with the same y-limits
plot(avg_parest2, xlab = "Ranks", ylab = "", cex.lab = 1.7, cex.axis = 1.2,
ylim = c(ymin, ymax))
# Add shared y-axis label
mtext("Scores", side = 2, outer = TRUE, line = 2.5, cex = 2)
library(MIDASim)
library(HMP2Data)
IBD16S()
count.ibd = t(IBD16S_mtx[, colSums(IBD16S_mtx)>3000] )
count.ibd = count.ibd[, colSums(count.ibd>0)>1]
dim(count.ibd)
View(IBD16S_samp)
uniqe(IBD16S_samp$diagnosis)
summary(IBD16S_samp$diagnosis)
summary(IBD16S_samp$diagnosis)
IBD16S_samp$diagnosis
View(IBD16S_tax)
devtools::document()
devtools::document()
devtools::document()
library(devtools)
devtools::document()
devtools::document()
devtools::document()
devtools::document()
